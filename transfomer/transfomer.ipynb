{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Is All You Need (2017) 실습\n",
    "\n",
    "본 코드는 논문 Attention Is All You Need(2017)을 리뷰한 이후 transfomer의 아키텍처를 더욱 이해하고자 실제 실습한 내용을 담고 있다.   \n",
    "리뷰 내용 :    \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* ref : https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice/blob/master/code_practices/Attention_is_All_You_Need_Tutorial_(German_English).ipynb / https://happy-jihye.github.io/nlp/nlp-8/ / https://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#torchtext'\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install torchtext==0.8.0 #torchtext 라이브러리 : 텍스트 처리 및 데이터 셋 로딩을 위해 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install spacy #자연어처리를 위한 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 393.8 kB/s eta 0:00:33\n",
      "     --------------------------------------- 0.1/12.8 MB 655.4 kB/s eta 0:00:20\n",
      "      --------------------------------------- 0.3/12.8 MB 1.5 MB/s eta 0:00:09\n",
      "     -- ------------------------------------- 0.8/12.8 MB 3.8 MB/s eta 0:00:04\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 6.6 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 4.3/12.8 MB 13.9 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 8.0/12.8 MB 22.3 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.9/12.8 MB 25.2 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 10.5/12.8 MB 36.4 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 10.9/12.8 MB 36.4 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 11.4/12.8 MB 38.6 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 11.9/12.8 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 12.4/12.8 MB 29.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.8 MB 28.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.8 MB 28.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.8 MB 28.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.8 MB 28.5 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 20.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (70.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sdlpc187\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.6.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\sdlpc187\\appdata\\roaming\\python\\python312\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\sdlpc187\\appdata\\roaming\\python\\python312\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting de-core-news-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n",
      "     ---------------------------------------- 0.0/14.6 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/14.6 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/14.6 MB 326.8 kB/s eta 0:00:45\n",
      "     --------------------------------------- 0.1/14.6 MB 798.9 kB/s eta 0:00:19\n",
      "     - -------------------------------------- 0.4/14.6 MB 2.2 MB/s eta 0:00:07\n",
      "     -- ------------------------------------- 0.9/14.6 MB 3.9 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 2.3/14.6 MB 8.6 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 2.9/14.6 MB 9.1 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 3.4/14.6 MB 9.5 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 3.9/14.6 MB 9.9 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 4.4/14.6 MB 9.9 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 4.9/14.6 MB 10.3 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 5.3/14.6 MB 10.2 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 5.9/14.6 MB 10.7 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 6.5/14.6 MB 10.6 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 7.0/14.6 MB 10.7 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 7.5/14.6 MB 10.9 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 8.1/14.6 MB 11.0 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 8.7/14.6 MB 10.9 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 9.2/14.6 MB 11.1 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 9.7/14.6 MB 11.1 MB/s eta 0:00:01\n",
      "     --------------------------- ----------- 10.3/14.6 MB 12.6 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 10.8/14.6 MB 13.4 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 11.3/14.6 MB 13.4 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 11.9/14.6 MB 12.6 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 12.3/14.6 MB 12.4 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 12.9/14.6 MB 12.1 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 13.3/14.6 MB 12.1 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 13.9/14.6 MB 12.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  14.5/14.6 MB 11.9 MB/s eta 0:00:01\n",
      "     --------------------------------------  14.6/14.6 MB 12.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  14.6/14.6 MB 12.1 MB/s eta 0:00:01\n",
      "     --------------------------------------- 14.6/14.6 MB 10.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from de-core-news-sm==3.7.0) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.7.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (70.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sdlpc187\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.18.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2024.6.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\sdlpc187\\appdata\\roaming\\python\\python312\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\sdlpc187\\appdata\\roaming\\python\\python312\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.18.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\sdlpc187\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import Multi30k\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# spaCy 모델 다운로드\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm\n",
    "\n",
    "# spaCy 모델 로드\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "\n",
    "# 토큰화 함수 정의\n",
    "def tokenize_en(text):\n",
    "    return [token.text for token in spacy_en.tokenizer(text)]\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [token.text for token in spacy_de.tokenizer(text)]\n",
    "\n",
    "# 토크나이저 정의\n",
    "tokenizer_de = get_tokenizer(tokenize_de)\n",
    "tokenizer_en = get_tokenizer(tokenize_en)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '.data\\\\multi30k\\\\train.de'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[126], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Multi30k\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 데이터셋 다운로드\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m train_data, valid_data, test_data \u001b[38;5;241m=\u001b[39m \u001b[43mMulti30k\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.de\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.en\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSRC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTRG\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of training examples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_data\u001b[38;5;241m.\u001b[39mexamples)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of validation examples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(valid_data\u001b[38;5;241m.\u001b[39mexamples)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\SDLPC187\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchtext\\datasets\\translation.py:113\u001b[0m, in \u001b[0;36mMulti30k.splits\u001b[1;34m(cls, exts, fields, root, train, validation, test, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m     path \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mMulti30k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SDLPC187\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchtext\\datasets\\translation.py:65\u001b[0m, in \u001b[0;36mTranslationDataset.splits\u001b[1;34m(cls, exts, fields, path, root, train, validation, test, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     63\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mdownload(root)\n\u001b[1;32m---> 65\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m train \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m val_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m validation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[0;32m     68\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, validation), exts, fields, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     69\u001b[0m test_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m test \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[0;32m     70\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, test), exts, fields, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\SDLPC187\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchtext\\datasets\\translation.py:34\u001b[0m, in \u001b[0;36mTranslationDataset.__init__\u001b[1;34m(self, path, exts, fields, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m src_path, trg_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(path \u001b[38;5;241m+\u001b[39m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m exts)\n\u001b[0;32m     33\u001b[0m examples \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m src_file, \\\n\u001b[0;32m     35\u001b[0m         io\u001b[38;5;241m.\u001b[39mopen(trg_path, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m trg_file:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m src_line, trg_line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(src_file, trg_file):\n\u001b[0;32m     37\u001b[0m         src_line, trg_line \u001b[38;5;241m=\u001b[39m src_line\u001b[38;5;241m.\u001b[39mstrip(), trg_line\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '.data\\\\multi30k\\\\train.de'"
     ]
    }
   ],
   "source": [
    "from torchtext.datasets import Multi30k\n",
    "\n",
    "# 데이터셋 다운로드\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'), \n",
    "                                                    fields=(SRC, TRG))\n",
    "\n",
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")\n",
    "\n",
    "\n",
    "from torchtext.data import Field\n",
    "\n",
    "# 필드 객체 생성\n",
    "SRC = Field(tokenize = tokenize_de, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import Multi30k\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# spaCy 모델 다운로드\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm\n",
    "\n",
    "# spaCy 모델 로드\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "\n",
    "# 토큰화 함수 정의\n",
    "def tokenize_en(text):\n",
    "    return [token.text for token in spacy_en.tokenizer(text)]\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [token.text for token in spacy_de.tokenizer(text)]\n",
    "\n",
    "# 토크나이저 정의\n",
    "tokenizer_de = get_tokenizer(tokenize_de)\n",
    "tokenizer_en = get_tokenizer(tokenize_en)\n",
    "\n",
    "SRC = Field(tokenize= tokenize_de,\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = True,\n",
    "            batch_first = True)\n",
    "\n",
    "TRG = Field(tokenize= tokenize_en,\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = True,\n",
    "            batch_first = True)\n",
    "\n",
    "# Multi30k 데이터셋 로드\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts= ('en', 'de'))\n",
    "\n",
    "# 단어 집합 빌드 함수\n",
    "def yield_tokens(data_iter, tokenizer, index):\n",
    "    for data_sample in data_iter:\n",
    "        yield tokenizer(data_sample[index])\n",
    "\n",
    "# 단어 집합 빌드\n",
    "vocab_de = build_vocab_from_iterator(yield_tokens(train_iter, tokenizer_de, index=0), specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "vocab_en = build_vocab_from_iterator(yield_tokens(train_iter, tokenizer_en, index=1), specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "\n",
    "# 단어 집합 설정\n",
    "vocab_de.set_default_index(vocab_de[\"<unk>\"])\n",
    "vocab_en.set_default_index(vocab_en[\"<unk>\"])\n",
    "\n",
    "# 데이터 로드 확인\n",
    "train_examples = list(train_iter)\n",
    "valid_examples = list(valid_iter)\n",
    "test_examples = list(test_iter)\n",
    "\n",
    "print(f\"Number of training examples: {len(train_examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_examples)}\")\n",
    "\n",
    "# 데이터 로더 정의\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(torch.tensor([vocab_de[token] for token in tokenizer_de(src_sample)], dtype=torch.long))\n",
    "        tgt_batch.append(torch.tensor([vocab_en[token] for token in tokenizer_en(tgt_sample)], dtype=torch.long))\n",
    "    src_batch = pad_sequence(src_batch, padding_value=vocab_de[\"<pad>\"])\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=vocab_en[\"<pad>\"])\n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, batch_size=32, collate_fn=collate_fn)\n",
    "valid_dataloader = DataLoader(valid_examples, batch_size=32, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_examples, batch_size=32, collate_fn=collate_fn)\n",
    "\n",
    "# 예시 데이터 토큰화 및 인덱싱\n",
    "example_text_de = \"Ein Beispieltext auf Deutsch.\"\n",
    "example_text_en = \"An example text in English.\"\n",
    "\n",
    "tokenized_de = tokenize_de(example_text_de)\n",
    "tokenized_en = tokenize_en(example_text_en)\n",
    "\n",
    "indexed_de = [vocab_de[token] for token in tokenized_de]\n",
    "indexed_en = [vocab_en[token] for token in tokenized_en]\n",
    "\n",
    "print(f\"Tokenized DE: {tokenized_de}\")\n",
    "print(f\"Indexed DE: {indexed_de}\")\n",
    "print(f\"Tokenized EN: {tokenized_en}\")\n",
    "print(f\"Indexed EN: {indexed_en}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. Multi Head Attention 아키텍처   \n",
    "- 어텐션의 input : 쿼리, 키, 값 - 모든 차원이 동일\n",
    "- 하이퍼 파라미터 : hidden_dim(하나의 단어에 대한 임베딩 차원), n_heads(= scaled dot-product attention의 개수), dropout_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads, dropout_ratio, device):\n",
    "        super().__init__()\n",
    "\n",
    "        assert hidden_dim % n_heads == 0\n",
    "\n",
    "        self.hidden_dim = hidden_dim # 임베딩 차원\n",
    "        self.n_heads = n_heads # 헤드(head)의 개수: 서로 다른 어텐션(attention) 컨셉의 수\n",
    "        self.head_dim = hidden_dim // n_heads # 각 헤드(head)에서의 임베딩 차원\n",
    "\n",
    "        self.fc_q = nn.Linear(hidden_dim, hidden_dim) # Query 값에 적용될 FC 레이어\n",
    "        self.fc_k = nn.Linear(hidden_dim, hidden_dim) # Key 값에 적용될 FC 레이어\n",
    "        self.fc_v = nn.Linear(hidden_dim, hidden_dim) # Value 값에 적용될 FC 레이어\n",
    "\n",
    "        self.fc_o = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "\n",
    "    def forward(self, query, key, value, mask = None):\n",
    "\n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        # query: [batch_size, query_len, hidden_dim]\n",
    "        # key: [batch_size, key_len, hidden_dim]\n",
    "        # value: [batch_size, value_len, hidden_dim]\n",
    " \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "\n",
    "        # Q: [batch_size, query_len, hidden_dim]\n",
    "        # K: [batch_size, key_len, hidden_dim]\n",
    "        # V: [batch_size, value_len, hidden_dim]\n",
    "\n",
    "        # hidden_dim → n_heads X head_dim 형태로 변형\n",
    "        # n_heads(h)개의 서로 다른 어텐션(attention) 컨셉을 학습하도록 유도\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        # Q: [batch_size, n_heads, query_len, head_dim]\n",
    "        # K: [batch_size, n_heads, key_len, head_dim]\n",
    "        # V: [batch_size, n_heads, value_len, head_dim]\n",
    "\n",
    "        # Attention Energy 계산\n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "\n",
    "        # energy: [batch_size, n_heads, query_len, key_len]\n",
    "\n",
    "        # 마스크(mask)를 사용하는 경우\n",
    "        if mask is not None:\n",
    "            # 마스크(mask) 값이 0인 부분을 -1e10으로 채우기\n",
    "            energy = energy.masked_fill(mask==0, -1e10)\n",
    "\n",
    "        # 어텐션(attention) 스코어 계산: 각 단어에 대한 확률 값\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "\n",
    "        # attention: [batch_size, n_heads, query_len, key_len]\n",
    "\n",
    "        # 여기에서 Scaled Dot-Product Attention을 계산\n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "\n",
    "        # x: [batch_size, n_heads, query_len, head_dim]\n",
    "\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "        # x: [batch_size, query_len, n_heads, head_dim]\n",
    "\n",
    "        x = x.view(batch_size, -1, self.hidden_dim)\n",
    "\n",
    "        # x: [batch_size, query_len, hidden_dim]\n",
    "\n",
    "        x = self.fc_o(x)\n",
    "\n",
    "        # x: [batch_size, query_len, hidden_dim]\n",
    "\n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. Position-wise Feedforward 아키텍처   \n",
    "- 입력과 출력의 차원 동일   \n",
    "- 하이퍼 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, pf_dim, dropout_ratio):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc_1 = nn.Linear(hidden_dim, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hidden_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # x: [batch_size, seq_len, hidden_dim]\n",
    "\n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "\n",
    "        # x: [batch_size, seq_len, pf_dim]\n",
    "\n",
    "        x = self.fc_2(x)\n",
    "\n",
    "        # x: [batch_size, seq_len, hidden_dim]\n",
    "\n",
    "        return x\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C. 인코더 레이어 아키텍처"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads, pf_dim, dropout_ratio, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hidden_dim, pf_dim, dropout_ratio)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    # 하나의 임베딩이 복제되어 Query, Key, Value로 입력되는 방식\n",
    "    def forward(self, src, src_mask):\n",
    "\n",
    "        # src: [batch_size, src_len, hidden_dim]\n",
    "        # src_mask: [batch_size, src_len]\n",
    "\n",
    "        # self attention\n",
    "        # 필요한 경우 마스크(mask) 행렬을 이용하여 어텐션(attention)할 단어를 조절 가능\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "\n",
    "        # dropout, residual connection and layer norm\n",
    "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "\n",
    "        # src: [batch_size, src_len, hidden_dim]\n",
    "\n",
    "        # position-wise feedforward\n",
    "        _src = self.positionwise_feedforward(src)\n",
    "\n",
    "        # dropout, residual and layer norm\n",
    "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
    "\n",
    "        # src: [batch_size, src_len, hidden_dim]\n",
    "\n",
    "        return src\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, device, max_length=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hidden_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hidden_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([EncoderLayer(hidden_dim, n_heads, pf_dim, dropout_ratio, device) for _ in range(n_layers)])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "\n",
    "        # src: [batch_size, src_len]\n",
    "        # src_mask: [batch_size, src_len]\n",
    "\n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "\n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "\n",
    "        # pos: [batch_size, src_len]\n",
    "\n",
    "        # 소스 문장의 임베딩과 위치 임베딩을 더한 것을 사용\n",
    "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "\n",
    "        # src: [batch_size, src_len, hidden_dim]\n",
    "\n",
    "        # 모든 인코더 레이어를 차례대로 거치면서 순전파(forward) 수행\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "\n",
    "        # src: [batch_size, src_len, hidden_dim]\n",
    "\n",
    "        return src # 마지막 레이어의 출력을 반환\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D. 디코더 레이어 아키텍처"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads, pf_dim, dropout_ratio, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.enc_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\n",
    "        self.encoder_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hidden_dim, pf_dim, dropout_ratio)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    # 인코더의 출력 값(enc_src)을 어텐션(attention)하는 구조\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "\n",
    "        # trg: [batch_size, trg_len, hidden_dim]\n",
    "        # enc_src: [batch_size, src_len, hidden_dim]\n",
    "        # trg_mask: [batch_size, trg_len]\n",
    "        # src_mask: [batch_size, src_len]\n",
    "\n",
    "        # self attention\n",
    "        # 자기 자신에 대하여 어텐션(attention)\n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "\n",
    "        # dropout, residual connection and layer norm\n",
    "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        # trg: [batch_size, trg_len, hidden_dim]\n",
    "\n",
    "        # encoder attention\n",
    "        # 디코더의 쿼리(Query)를 이용해 인코더를 어텐션(attention)\n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "\n",
    "        # dropout, residual connection and layer norm\n",
    "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        # trg: [batch_size, trg_len, hidden_dim]\n",
    "\n",
    "        # positionwise feedforward\n",
    "        _trg = self.positionwise_feedforward(trg)\n",
    "\n",
    "        # dropout, residual and layer norm\n",
    "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        # trg: [batch_size, trg_len, hidden_dim]\n",
    "        # attention: [batch_size, n_heads, trg_len, src_len]\n",
    "\n",
    "        return trg, attention\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, device, max_length=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.tok_embedding = nn.Embedding(output_dim, hidden_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hidden_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(hidden_dim, n_heads, pf_dim, dropout_ratio, device) for _ in range(n_layers)])\n",
    "\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "\n",
    "        # trg: [batch_size, trg_len]\n",
    "        # enc_src: [batch_size, src_len, hidden_dim]\n",
    "        # trg_mask: [batch_size, trg_len]\n",
    "        # src_mask: [batch_size, src_len]\n",
    "\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "\n",
    "        # pos: [batch_size, trg_len]\n",
    "\n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "\n",
    "        # trg: [batch_size, trg_len, hidden_dim]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            # 소스 마스크와 타겟 마스크 모두 사용\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        # trg: [batch_size, trg_len, hidden_dim]\n",
    "        # attention: [batch_size, n_heads, trg_len, src_len]\n",
    "\n",
    "        output = self.fc_out(trg)\n",
    "\n",
    "        # output: [batch_size, trg_len, output_dim]\n",
    "\n",
    "        return output, attention\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E. 트랜스포머 아키텍처"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    # 소스 문장의  토큰에 대하여 마스크(mask) 값을 0으로 설정\n",
    "    def make_src_mask(self, src):\n",
    "\n",
    "        # src: [batch_size, src_len]\n",
    "\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # src_mask: [batch_size, 1, 1, src_len]\n",
    "\n",
    "        return src_mask\n",
    "\n",
    "    # 타겟 문장에서 각 단어는 다음 단어가 무엇인지 알 수 없도록(이전 단어만 보도록) 만들기 위해 마스크를 사용\n",
    "    def make_trg_mask(self, trg):\n",
    "\n",
    "        # trg: [batch_size, trg_len]\n",
    "\n",
    "        \"\"\" (마스크 예시)\n",
    "        1 0 0 0 0\n",
    "        1 1 0 0 0\n",
    "        1 1 1 0 0\n",
    "        1 1 1 0 0\n",
    "        1 1 1 0 0\n",
    "        \"\"\"\n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # trg_pad_mask: [batch_size, 1, 1, trg_len]\n",
    "\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        \"\"\" (마스크 예시)\n",
    "        1 0 0 0 0\n",
    "        1 1 0 0 0\n",
    "        1 1 1 0 0\n",
    "        1 1 1 1 0\n",
    "        1 1 1 1 1\n",
    "        \"\"\"\n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "\n",
    "        # trg_sub_mask: [trg_len, trg_len]\n",
    "\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "\n",
    "        # trg_mask: [batch_size, 1, trg_len, trg_len]\n",
    "\n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "\n",
    "        # src: [batch_size, src_len]\n",
    "        # trg: [batch_size, trg_len]\n",
    "\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "\n",
    "        # src_mask: [batch_size, 1, 1, src_len]\n",
    "        # trg_mask: [batch_size, 1, trg_len, trg_len]\n",
    "\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "\n",
    "        # enc_src: [batch_size, src_len, hidden_dim]\n",
    "\n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        # output: [batch_size, trg_len, output_dim]\n",
    "        # attention: [batch_size, n_heads, trg_len, src_len]\n",
    "\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습(Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Field' object has no attribute 'vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m INPUT_DIM \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mSRC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m)\n\u001b[0;32m      2\u001b[0m OUTPUT_DIM \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(TRG\u001b[38;5;241m.\u001b[39mvocab)\n\u001b[0;32m      3\u001b[0m HIDDEN_DIM \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Field' object has no attribute 'vocab'"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "HIDDEN_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "\n",
    "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "# 인코더(encoder)와 디코더(decoder) 객체 선언\n",
    "enc = Encoder(INPUT_DIM, HIDDEN_DIM, ENC_LAYERS, ENC_HEADS, ENC_PF_DIM, ENC_DROPOUT, device)\n",
    "dec = Decoder(OUTPUT_DIM, HIDDEN_DIM, DEC_LAYERS, DEC_HEADS, DEC_PF_DIM, DEC_DROPOUT, device)\n",
    "\n",
    "# Transformer 객체 선언\n",
    "model = Transformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
